{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Kaggle download + imports"
      ],
      "metadata": {
        "id": "Y2pkOPiJuRh4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxknFhWGt_zv",
        "outputId": "7ef29e4e-13fd-4eae-afd5-b9fec0167e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "!pip -q install kaggle\n",
        "\n",
        "import os, json, shutil, random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"GPU:\", tf.config.list_physical_devices(\"GPU\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Download CelebA from Kaggle"
      ],
      "metadata": {
        "id": "anWoPWe5uYIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install kaggle\n",
        "\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "from getpass import getpass\n",
        "\n",
        "KAGGLE_USERNAME = input(\"Kaggle username: \")\n",
        "KAGGLE_KEY = getpass(\"Kaggle API key (hidden): \")\n",
        "\n",
        "kaggle_dir = Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
        "with open(kaggle_json_path, \"w\") as f:\n",
        "    json.dump({\"username\": KAGGLE_USERNAME, \"key\": KAGGLE_KEY}, f)\n",
        "\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "print(\"✅ kaggle.json created at:\", kaggle_json_path)\n",
        "\n",
        "data_root = Path(\"/content/celeba_kaggle\")\n",
        "data_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset -p {data_root} --unzip\n",
        "!ls -lah {data_root}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bECyj4jKueQb",
        "outputId": "bbeca372-e381-457f-d7bb-8529ab0b4cac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle username: prithilasaha\n",
            "Kaggle API key (hidden): ··········\n",
            "✅ kaggle.json created at: /root/.kaggle/kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\n",
            "License(s): other\n",
            "Downloading celeba-dataset.zip to /content/celeba_kaggle\n",
            " 98% 1.30G/1.33G [00:32<00:00, 62.2MB/s]\n",
            "100% 1.33G/1.33G [00:32<00:00, 44.2MB/s]\n",
            "total 45M\n",
            "drwxr-xr-x 3 root root 4.0K Jan  7 23:18 .\n",
            "drwxr-xr-x 1 root root 4.0K Jan  7 23:13 ..\n",
            "-rw-r--r-- 1 root root 3.3M Jan  7 23:13 identity_CelebA.txt\n",
            "drwxr-xr-x 3 root root 4.0K Jan  7 23:11 img_align_celeba\n",
            "-rw-r--r-- 1 root root  24M Jan  7 23:18 list_attr_celeba.csv\n",
            "-rw-r--r-- 1 root root 5.2M Jan  7 23:18 list_bbox_celeba.csv\n",
            "-rw-r--r-- 1 root root 2.8M Jan  7 23:18 list_eval_partition.csv\n",
            "-rw-r--r-- 1 root root 9.5M Jan  7 23:18 list_landmarks_align_celeba.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Locate image folder + identity labels"
      ],
      "metadata": {
        "id": "wLHacXw4u-Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "data_root = Path(\"/content/celeba_kaggle\")\n",
        "\n",
        "# Find image folder\n",
        "img_dir = None\n",
        "for cand in [\"img_align_celeba\", \"img_align_celeba/img_align_celeba\"]:\n",
        "    hits = list(data_root.rglob(cand))\n",
        "    if hits:\n",
        "        img_dir = hits[0]\n",
        "        break\n",
        "\n",
        "print(\"img_dir:\", img_dir)\n",
        "if img_dir is None:\n",
        "    raise FileNotFoundError(\"Could not find img_align_celeba folder inside your dataset.\")\n",
        "\n",
        "# Find identity file\n",
        "identity_path = None\n",
        "hits = list(data_root.rglob(\"identity_CelebA.txt\"))\n",
        "if hits:\n",
        "    identity_path = hits[0]\n",
        "\n",
        "if identity_path is None:\n",
        "    identity_path = data_root / \"identity_CelebA.txt\"\n",
        "    !wget -q -O \"{identity_path}\" https://raw.githubusercontent.com/Golbstein/keras-face-recognition/master/identity_CelebA.txt\n",
        "    print(\"Downloaded identity file to:\", identity_path)\n",
        "\n",
        "# Sanity check\n",
        "if not identity_path.exists() or identity_path.stat().st_size < 1000:\n",
        "    raise FileNotFoundError(\"identity_CelebA.txt is missing or too small; download failed.\")\n",
        "\n",
        "print(\"identity_path:\", identity_path)\n",
        "print(\"identity file size (bytes):\", identity_path.stat().st_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FwPQJxOue_e",
        "outputId": "15a9c28b-338a-448c-dd07-7a9bda7dc795"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_dir: /content/celeba_kaggle/img_align_celeba\n",
            "identity_path: /content/celeba_kaggle/identity_CelebA.txt\n",
            "identity file size (bytes): 3424458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Dataset Splitting"
      ],
      "metadata": {
        "id": "_52Fxx5fvJLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, random, json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "SEED=42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "TARGET_CLASSES=10\n",
        "IMAGES_PER_CLASS=25\n",
        "TRAIN_PER_CLASS=20\n",
        "VAL_PER_CLASS=5\n",
        "\n",
        "data_root = Path(\"/content/celeba_kaggle\")\n",
        "\n",
        "# 1) Find the directory that DIRECTLY contains .jpg files (not just recursively)\n",
        "all_dirs = [p for p in data_root.rglob(\"*\") if p.is_dir()]\n",
        "\n",
        "def direct_jpg_count(d: Path) -> int:\n",
        "    return len(list(d.glob(\"*.jpg\")))\n",
        "\n",
        "jpg_dirs = [(d, direct_jpg_count(d)) for d in all_dirs]\n",
        "jpg_dirs = [x for x in jpg_dirs if x[1] > 0]\n",
        "\n",
        "if not jpg_dirs:\n",
        "    raise FileNotFoundError(\"❌ No directory with direct *.jpg files found under /content/celeba_kaggle\")\n",
        "\n",
        "img_dir, n_jpg = max(jpg_dirs, key=lambda x: x[1])\n",
        "print(\"✅ img_dir (direct jpg folder):\", img_dir)\n",
        "print(\"✅ direct jpg count:\", n_jpg)\n",
        "\n",
        "# 2) Identity file must exist and be non-empty\n",
        "identity_path = data_root / \"identity_CelebA.txt\"\n",
        "if (not identity_path.exists()) or identity_path.stat().st_size < 1000:\n",
        "    print(\"\\n⚠️ identity_CelebA.txt missing/empty -> upload it now\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # upload identity_CelebA.txt\n",
        "    src = Path(\"identity_CelebA.txt\")\n",
        "    if not src.exists():\n",
        "        raise FileNotFoundError(\"Upload must be named exactly identity_CelebA.txt\")\n",
        "    shutil.copy2(src, identity_path)\n",
        "    print(\"✅ Uploaded identity file bytes:\", identity_path.stat().st_size)\n",
        "\n",
        "# 3) Load identity file safely\n",
        "raw = pd.read_csv(identity_path, sep=r\"\\s+\", header=None)\n",
        "df = raw.iloc[:, :2].copy()\n",
        "df.columns = [\"filename\", \"identity\"]\n",
        "df[\"filename\"] = df[\"filename\"].astype(str)\n",
        "df[\"identity\"] = df[\"identity\"].astype(int)\n",
        "\n",
        "# 4) Fast match: keep only filenames that exist in img_dir\n",
        "existing = set(p.name for p in img_dir.glob(\"*.jpg\"))\n",
        "df = df[df[\"filename\"].isin(existing)].copy()\n",
        "print(\"✅ rows matched with existing images:\", len(df))\n",
        "\n",
        "if len(df) == 0:\n",
        "    print(\"Example filenames from identity file:\", raw.iloc[:5,0].tolist())\n",
        "    print(\"Example filenames in img_dir:\", list(existing)[:5])\n",
        "    raise ValueError(\"❌ Still zero matches. Your identity file doesn't match your image filenames.\")\n",
        "\n",
        "# 5) Pick identities with enough images\n",
        "counts = df[\"identity\"].value_counts()\n",
        "eligible = counts[counts >= IMAGES_PER_CLASS]\n",
        "print(\"✅ eligible identities:\", len(eligible), \"with >=\", IMAGES_PER_CLASS, \"images\")\n",
        "\n",
        "if len(eligible) < TARGET_CLASSES:\n",
        "    raise ValueError(f\"❌ Not enough identities with >= {IMAGES_PER_CLASS} images. Try IMAGES_PER_CLASS=20\")\n",
        "\n",
        "top_ids = eligible.head(TARGET_CLASSES).index.tolist()\n",
        "id_map = {ident:i for i, ident in enumerate(top_ids)}\n",
        "df_small = df[df[\"identity\"].isin(top_ids)].copy()\n",
        "df_small[\"class_idx\"] = df_small[\"identity\"].map(id_map)\n",
        "\n",
        "# 6) Sample IMAGES_PER_CLASS per class and copy into reduced_celeba\n",
        "samples=[]\n",
        "for cls, g in df_small.groupby(\"class_idx\"):\n",
        "    samples.append(g.sample(n=IMAGES_PER_CLASS, random_state=SEED))\n",
        "df_sampled = pd.concat(samples, ignore_index=True)\n",
        "\n",
        "out_root = Path(\"/content/reduced_celeba\")\n",
        "if out_root.exists(): shutil.rmtree(out_root)\n",
        "out_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def cname(i): return f\"person_{i:03d}\"\n",
        "for i in range(TARGET_CLASSES):\n",
        "    (out_root / cname(i)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for _, r in df_sampled.iterrows():\n",
        "    src = img_dir / r[\"filename\"]\n",
        "    dst = out_root / cname(int(r[\"class_idx\"])) / r[\"filename\"]\n",
        "    shutil.copy2(src, dst)\n",
        "\n",
        "with open(\"/content/class_mapping.json\",\"w\") as f:\n",
        "    json.dump({cname(v): int(k) for k,v in id_map.items()}, f, indent=2)\n",
        "\n",
        "print(\"✅ reduced_celeba total jpg:\", sum(1 for _ in out_root.rglob(\"*.jpg\")))\n",
        "\n",
        "# 7) Build balanced split\n",
        "split_root = Path(\"/content/reduced_celeba_split\")\n",
        "if split_root.exists(): shutil.rmtree(split_root)\n",
        "train_root = split_root/\"train\"; val_root = split_root/\"val\"\n",
        "train_root.mkdir(parents=True, exist_ok=True); val_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "class_dirs = sorted([d for d in out_root.iterdir() if d.is_dir()])\n",
        "for cls_dir in class_dirs:\n",
        "    imgs = sorted(list(cls_dir.glob(\"*.jpg\")))\n",
        "    random.shuffle(imgs)\n",
        "    need = TRAIN_PER_CLASS + VAL_PER_CLASS\n",
        "    if len(imgs) < need:\n",
        "        raise ValueError(f\"❌ {cls_dir.name} has only {len(imgs)} images. Need {need}.\")\n",
        "    train_imgs = imgs[:TRAIN_PER_CLASS]\n",
        "    val_imgs = imgs[TRAIN_PER_CLASS:TRAIN_PER_CLASS+VAL_PER_CLASS]\n",
        "\n",
        "    (train_root/cls_dir.name).mkdir(parents=True, exist_ok=True)\n",
        "    (val_root/cls_dir.name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for p in train_imgs: shutil.copy2(p, train_root/cls_dir.name/p.name)\n",
        "    for p in val_imgs: shutil.copy2(p, val_root/cls_dir.name/p.name)\n",
        "\n",
        "print(\"✅ split train images:\", sum(1 for _ in train_root.rglob(\"*.jpg\")))\n",
        "print(\"✅ split val images:\", sum(1 for _ in val_root.rglob(\"*.jpg\")))\n",
        "print(\"✅ READY for Task 2 at:\", split_root)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v485RD4QvNZB",
        "outputId": "6e2be0ea-cabf-4af7-fb67-14dcbb868aa0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ img_dir (direct jpg folder): /content/celeba_kaggle/img_align_celeba/img_align_celeba\n",
            "✅ direct jpg count: 202599\n",
            "✅ rows matched with existing images: 202599\n",
            "✅ eligible identities: 3661 with >= 25 images\n",
            "✅ reduced_celeba total jpg: 250\n",
            "✅ split train images: 200\n",
            "✅ split val images: 50\n",
            "✅ READY for Task 2 at: /content/reduced_celeba_split\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: VGG16 Model Training + Fine-Tuning"
      ],
      "metadata": {
        "id": "D9PDu-fY0O_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DIR = Path(\"/content/reduced_celeba_split/train\")\n",
        "VAL_DIR   = Path(\"/content/reduced_celeba_split/val\")\n",
        "\n",
        "# quick checks\n",
        "print(\"TRAIN_DIR exists:\", TRAIN_DIR.exists())\n",
        "print(\"VAL_DIR exists:\", VAL_DIR.exists())\n",
        "train_imgs = sum(1 for _ in TRAIN_DIR.rglob(\"*.jpg\"))\n",
        "val_imgs   = sum(1 for _ in VAL_DIR.rglob(\"*.jpg\"))\n",
        "print(\"Train jpg:\", train_imgs, \"| Val jpg:\", val_imgs)\n",
        "!find /content/reduced_celeba_split -maxdepth 2 -type d | head -n 30\n",
        "\n",
        "if train_imgs == 0 or val_imgs == 0:\n",
        "    raise ValueError(\"No images found in train/val. Rebuild reduced_celeba_split first.\")\n",
        "\n",
        "# VGG16 expects 224x224\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16\n",
        "SEED = 42\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    str(TRAIN_DIR),\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    str(VAL_DIR),\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "NUM_CLASSES = len(train_ds.class_names)\n",
        "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
        "print(\"Class names:\", train_ds.class_names)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
        "\n",
        "# mild augmentation\n",
        "aug = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.02),\n",
        "    layers.RandomZoom(0.05),\n",
        "], name=\"aug\")\n",
        "\n",
        "# VGG16 backbone\n",
        "base = keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=IMG_SIZE + (3,)\n",
        ")\n",
        "\n",
        "# Freeze all layers first\n",
        "for layer in base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Unfreeze last quarter of layers\n",
        "n = len(base.layers)\n",
        "start_unfreeze = int(n * 0.75)  # last 25%\n",
        "for layer in base.layers[start_unfreeze:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "print(\"VGG16 total layers:\", n)\n",
        "print(\"Unfrozen from index:\", start_unfreeze)\n",
        "print(\"Trainable layers:\", sum(l.trainable for l in base.layers))\n",
        "\n",
        "# Build model\n",
        "inputs = keras.Input(shape=IMG_SIZE + (3,))\n",
        "x = aug(inputs)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "x = base(x, training=True)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "model2 = keras.Model(inputs, outputs, name=\"task2_vgg16_finetune\")\n",
        "\n",
        "model2.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),  # small LR\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top5_acc\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"/content/task2_vgg16_best.keras\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=8, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.5, min_lr=1e-7, verbose=1),\n",
        "]\n",
        "\n",
        "history2 = model2.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Task 2 Eval:\", model2.evaluate(val_ds, verbose=1))\n",
        "\n",
        "final_path = \"/content/task2_vgg16_final.keras\"\n",
        "model2.save(final_path)\n",
        "print(\"✅ Saved:\", final_path)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/task2_vgg16_best.keras\")\n",
        "files.download(final_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wsk4S92tx035",
        "outputId": "9d35e5f8-6ae5-4f24-96c0-cdf520c0df71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_DIR exists: True\n",
            "VAL_DIR exists: True\n",
            "Train jpg: 200 | Val jpg: 50\n",
            "/content/reduced_celeba_split\n",
            "/content/reduced_celeba_split/train\n",
            "/content/reduced_celeba_split/train/person_001\n",
            "/content/reduced_celeba_split/train/person_006\n",
            "/content/reduced_celeba_split/train/person_008\n",
            "/content/reduced_celeba_split/train/person_002\n",
            "/content/reduced_celeba_split/train/person_007\n",
            "/content/reduced_celeba_split/train/person_004\n",
            "/content/reduced_celeba_split/train/person_005\n",
            "/content/reduced_celeba_split/train/person_000\n",
            "/content/reduced_celeba_split/train/person_009\n",
            "/content/reduced_celeba_split/train/person_003\n",
            "/content/reduced_celeba_split/val\n",
            "/content/reduced_celeba_split/val/person_001\n",
            "/content/reduced_celeba_split/val/person_006\n",
            "/content/reduced_celeba_split/val/person_008\n",
            "/content/reduced_celeba_split/val/person_002\n",
            "/content/reduced_celeba_split/val/person_007\n",
            "/content/reduced_celeba_split/val/person_004\n",
            "/content/reduced_celeba_split/val/person_005\n",
            "/content/reduced_celeba_split/val/person_000\n",
            "/content/reduced_celeba_split/val/person_009\n",
            "/content/reduced_celeba_split/val/person_003\n",
            "Found 200 files belonging to 10 classes.\n",
            "Found 50 files belonging to 10 classes.\n",
            "NUM_CLASSES: 10\n",
            "Class names: ['person_000', 'person_001', 'person_002', 'person_003', 'person_004', 'person_005', 'person_006', 'person_007', 'person_008', 'person_009']\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "VGG16 total layers: 19\n",
            "Unfrozen from index: 14\n",
            "Trainable layers: 5\n",
            "Epoch 1/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.0832 - loss: 5.7463 - top5_acc: 0.6321\n",
            "Epoch 1: val_accuracy improved from -inf to 0.20000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 312ms/step - accuracy: 0.0848 - loss: 5.6950 - top5_acc: 0.6280 - val_accuracy: 0.2000 - val_loss: 3.5289 - val_top5_acc: 0.5400 - learning_rate: 1.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1651 - loss: 3.2890 - top5_acc: 0.6030\n",
            "Epoch 2: val_accuracy improved from 0.20000 to 0.22000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.1665 - loss: 3.2752 - top5_acc: 0.6020 - val_accuracy: 0.2200 - val_loss: 2.7571 - val_top5_acc: 0.7200 - learning_rate: 1.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.1620 - loss: 2.7917 - top5_acc: 0.7004\n",
            "Epoch 3: val_accuracy improved from 0.22000 to 0.24000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - accuracy: 0.1637 - loss: 2.7817 - top5_acc: 0.6986 - val_accuracy: 0.2400 - val_loss: 2.3946 - val_top5_acc: 0.7000 - learning_rate: 1.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2618 - loss: 2.2304 - top5_acc: 0.7666\n",
            "Epoch 4: val_accuracy improved from 0.24000 to 0.28000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - accuracy: 0.2599 - loss: 2.2288 - top5_acc: 0.7657 - val_accuracy: 0.2800 - val_loss: 2.2097 - val_top5_acc: 0.7600 - learning_rate: 1.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2855 - loss: 2.0354 - top5_acc: 0.8154\n",
            "Epoch 5: val_accuracy did not improve from 0.28000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.2848 - loss: 2.0372 - top5_acc: 0.8136 - val_accuracy: 0.2600 - val_loss: 2.0890 - val_top5_acc: 0.7800 - learning_rate: 1.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4037 - loss: 1.9181 - top5_acc: 0.7896\n",
            "Epoch 6: val_accuracy improved from 0.28000 to 0.32000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 0.4016 - loss: 1.9156 - top5_acc: 0.7907 - val_accuracy: 0.3200 - val_loss: 1.9972 - val_top5_acc: 0.7800 - learning_rate: 1.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3688 - loss: 1.8090 - top5_acc: 0.8567\n",
            "Epoch 7: val_accuracy improved from 0.32000 to 0.38000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - accuracy: 0.3696 - loss: 1.8085 - top5_acc: 0.8566 - val_accuracy: 0.3800 - val_loss: 1.9152 - val_top5_acc: 0.8200 - learning_rate: 1.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4062 - loss: 1.7680 - top5_acc: 0.8897\n",
            "Epoch 8: val_accuracy improved from 0.38000 to 0.48000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - accuracy: 0.4061 - loss: 1.7649 - top5_acc: 0.8883 - val_accuracy: 0.4800 - val_loss: 1.8265 - val_top5_acc: 0.8200 - learning_rate: 1.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.4539 - loss: 1.6174 - top5_acc: 0.8934\n",
            "Epoch 9: val_accuracy improved from 0.48000 to 0.54000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 0.4540 - loss: 1.6141 - top5_acc: 0.8942 - val_accuracy: 0.5400 - val_loss: 1.7475 - val_top5_acc: 0.8400 - learning_rate: 1.0000e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6012 - loss: 1.3914 - top5_acc: 0.9091\n",
            "Epoch 10: val_accuracy improved from 0.54000 to 0.60000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 141ms/step - accuracy: 0.5990 - loss: 1.3935 - top5_acc: 0.9088 - val_accuracy: 0.6000 - val_loss: 1.6816 - val_top5_acc: 0.8600 - learning_rate: 1.0000e-05\n",
            "Epoch 11/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5589 - loss: 1.3904 - top5_acc: 0.9437\n",
            "Epoch 11: val_accuracy did not improve from 0.60000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.5608 - loss: 1.3876 - top5_acc: 0.9453 - val_accuracy: 0.6000 - val_loss: 1.6374 - val_top5_acc: 0.8800 - learning_rate: 1.0000e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6140 - loss: 1.1983 - top5_acc: 0.9748\n",
            "Epoch 12: val_accuracy improved from 0.60000 to 0.62000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.6170 - loss: 1.1986 - top5_acc: 0.9741 - val_accuracy: 0.6200 - val_loss: 1.6023 - val_top5_acc: 0.8800 - learning_rate: 1.0000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7371 - loss: 1.0174 - top5_acc: 0.9830\n",
            "Epoch 13: val_accuracy did not improve from 0.62000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.7366 - loss: 1.0179 - top5_acc: 0.9828 - val_accuracy: 0.6000 - val_loss: 1.5768 - val_top5_acc: 0.8800 - learning_rate: 1.0000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7761 - loss: 0.9928 - top5_acc: 0.9590\n",
            "Epoch 14: val_accuracy did not improve from 0.62000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.7756 - loss: 0.9960 - top5_acc: 0.9594 - val_accuracy: 0.6200 - val_loss: 1.5508 - val_top5_acc: 0.8800 - learning_rate: 1.0000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7140 - loss: 0.9516 - top5_acc: 0.9953\n",
            "Epoch 15: val_accuracy did not improve from 0.62000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.7137 - loss: 0.9568 - top5_acc: 0.9945 - val_accuracy: 0.6200 - val_loss: 1.5192 - val_top5_acc: 0.8800 - learning_rate: 1.0000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7836 - loss: 0.8961 - top5_acc: 0.9748\n",
            "Epoch 16: val_accuracy improved from 0.62000 to 0.64000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 144ms/step - accuracy: 0.7834 - loss: 0.8952 - top5_acc: 0.9752 - val_accuracy: 0.6400 - val_loss: 1.5037 - val_top5_acc: 0.8600 - learning_rate: 1.0000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8543 - loss: 0.7697 - top5_acc: 0.9927\n",
            "Epoch 17: val_accuracy improved from 0.64000 to 0.68000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 172ms/step - accuracy: 0.8522 - loss: 0.7723 - top5_acc: 0.9922 - val_accuracy: 0.6800 - val_loss: 1.4872 - val_top5_acc: 0.9000 - learning_rate: 1.0000e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8479 - loss: 0.8393 - top5_acc: 0.9666\n",
            "Epoch 18: val_accuracy did not improve from 0.68000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.8469 - loss: 0.8388 - top5_acc: 0.9672 - val_accuracy: 0.6800 - val_loss: 1.4530 - val_top5_acc: 0.9000 - learning_rate: 1.0000e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8663 - loss: 0.6904 - top5_acc: 0.9957\n",
            "Epoch 19: val_accuracy improved from 0.68000 to 0.72000, saving model to /content/task2_vgg16_best.keras\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - accuracy: 0.8659 - loss: 0.6925 - top5_acc: 0.9956 - val_accuracy: 0.7200 - val_loss: 1.4301 - val_top5_acc: 0.8800 - learning_rate: 1.0000e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8577 - loss: 0.6638 - top5_acc: 1.0000\n",
            "Epoch 20: val_accuracy did not improve from 0.72000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.8583 - loss: 0.6642 - top5_acc: 1.0000 - val_accuracy: 0.7200 - val_loss: 1.4100 - val_top5_acc: 0.8800 - learning_rate: 1.0000e-05\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7088 - loss: 1.4047 - top5_acc: 0.8874\n",
            "Task 2 Eval: [1.4300974607467651, 0.7200000286102295, 0.8799999952316284]\n",
            "✅ Saved: /content/task2_vgg16_final.keras\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e43d8156-8b2e-4f64-b7ca-5c880f828ded\", \"task2_vgg16_best.keras\", 115664061)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d1f652c0-b00d-4466-93ae-9f1d4a71e6a8\", \"task2_vgg16_final.keras\", 115664061)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
